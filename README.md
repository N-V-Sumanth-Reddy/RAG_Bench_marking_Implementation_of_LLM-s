# RAG_Bench_marking_Implementation_of_LLM-s
# RAGBench â€“ Gradio Interface for Benchmarking Retrieval-Augmented Generation (RAG) Systems

This project implements an interactive Gradio-based evaluation interface for [**RAGBench**](https://arxiv.org/abs/2407.11005), a large-scale, explainable benchmark designed to assess Retrieval-Augmented Generation (RAG) systems. It enables researchers and developers to evaluate LLM-based RAG pipelines using fine-grained, human-annotated metrics like **Groundedness**, **Faithfulness**, and **Coverage**.

---

## ğŸ“š About RAGBench

**RAGBench** is the first benchmark that provides explainable, domain-specific evaluations of RAG systems, going beyond simple answer correctness. It includes over **100,000 QA examples** curated from **five real-world domains**, each with labeled answers and retrievals to test LLM reliability in practical settings.

### âœ¨ Evaluation Dimensions

| Metric         | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| **Groundedness** | Measures whether the generated answer is directly supported by the retrieved context |
| **Faithfulness** | Assesses whether the model introduces hallucinated or unsupported information        |
| **Coverage**     | Evaluates how completely the answer captures all necessary points from the context   |

---

## ğŸ—‚ Dataset Overview

RAGBench contains QA data from the following **five real-world domains**:

- ğŸ“˜ Technical Manuals  
- ğŸ¦ Financial Documents  
- âš–ï¸ Compliance and Legal Texts  
- ğŸ“š Academic and Scientific Sources  
- ğŸŒ Open-domain QA  

Each sample in the dataset includes:
- A user query
- Retrieved documents
- An LLM-generated answer
- Human-annotated evaluations on the above dimensionss

---

## ğŸ’» Gradio Interface

The interactive app includes:

### ğŸ”¹ Section 1: Evaluate a RAG Pipeline
- Choose:
  - âœ… Dataset
  - ğŸ” Embedding Model (e.g., `text-embedding-ada-002`)
  - ğŸ’¬ QA Model (e.g., `gpt-3.5-turbo`, `Mistral`)
- Click **â€œGet Metricsâ€** to view:
  - Groundedness
  - Faithfulness
  - Coverage

### ğŸ”¹ Section 2: Custom Q&A (Live Testing)
- Input your own question
- Retrieve and display top context passages
- Generate an answer using the QA model

---


