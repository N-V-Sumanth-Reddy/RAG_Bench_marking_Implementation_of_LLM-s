# RAG_Bench_marking_Implementation_of_LLM-s
# RAGBench â€“ Gradio Interface for Benchmarking Retrieval-Augmented Generation (RAG) Systems

This project implements an interactive Gradio-based evaluation interface for [**RAGBench**](https://arxiv.org/abs/2407.11005), a large-scale, explainable benchmark designed to assess Retrieval-Augmented Generation (RAG) systems. It enables researchers and developers to evaluate LLM-based RAG pipelines using fine-grained, human-annotated metrics like **Groundedness**, **Faithfulness**, and **Coverage**.

---

## ğŸ“š About RAGBench

**RAGBench** is the first benchmark that provides explainable, domain-specific evaluations of RAG systems, going beyond simple answer correctness. It includes over **100,000 QA examples** curated from **five real-world domains**, each with labeled answers and retrievals to test LLM reliability in practical settings.

### âœ¨ Evaluation Dimensions

| Metric         | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| **Groundedness** | Measures whether the generated answer is directly supported by the retrieved context |
| **Faithfulness** | Assesses whether the model introduces hallucinated or unsupported information        |
| **Coverage**     | Evaluates how completely the answer captures all necessary points from the context   |

---

## ğŸ—‚ Dataset Overview

RAGBench contains QA data from the following **five real-world domains**:

- ğŸ“˜ Technical Manuals  
- ğŸ¦ Financial Documents  
- âš–ï¸ Compliance and Legal Texts  
- ğŸ“š Academic and Scientific Sources  
- ğŸŒ Open-domain QA  

Each sample in the dataset includes:
- A user query
- Retrieved documents
- An LLM-generated answer
- Human-annotated evaluations on the above dimensionss

---

## ğŸ’» Gradio Interface

The interactive app includes:

### ğŸ”¹ Section 1: Evaluate a RAG Pipeline
- Choose:
  - âœ… Dataset
  - ğŸ” Embedding Model (e.g., `text-embedding-ada-002`)
  - ğŸ’¬ QA Model (e.g., `gpt-3.5-turbo`, `Mistral`)
- Click **â€œGet Metricsâ€** to view:
  - Groundedness
  - Faithfulness
  - Coverage

### ğŸ”¹ Section 2: Custom Q&A (Live Testing)
- Input your own question
- Retrieve and display top context passages
- Generate an answer using the QA model

---
## ğŸš€ Usage
### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-repo/RAG_Bench_marking_Implementation_of_LLM-s.git
cd RGB_Benchmarking
```

### 2ï¸âƒ£ RUN the ipynb file

Run all the cells in the ipynb file so that you can use gradio app to evaluate in the last cell.


### 3ï¸âƒ£ Run the Evaluation

Initially please add your own API Keys of the LLM models in the config.ini file. Select the paramaters and model to evaluate the model will automatically fetch the data set required form the data config folder that is placed in the drive if the data config file dosen't exist in the drive please download and upload in the drive. Also select the input parameters in the gradio app.


## ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.

---

For more details, refer to the original research paper: [RAGBench: Explainable Benchmark for Retrievalâ€‘Augmented Generation Systems](https://arxiv.org/pdf/2407.11005)

